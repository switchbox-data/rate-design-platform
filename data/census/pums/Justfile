# PUMS Data Pipeline
#
# Downloads Census ACS PUMS zips, extracts to CSV, converts to Parquet.
#
# QUICK START:
#   just prepare                       # All states, both record types (defaults)
#   just state=NY prepare              # One state (fetch+unzip+convert in one invocation)
#
# INDIVIDUAL STEPS (override top-level vars: just survey=acs1 end_year=2024 state=DC fetch):
#   just fetch                         # Download to zips/
#   just unzip                         # Extract zips to csv/
#   just convert                       # Convert csv/ to parquet/
#   just upload                        # Sync parquet/ to s3://data.sb/census/pums/
#   just clean                         # Remove zips/, csv/, parquet/
#
# Run from repo root: just -f data/census/pums/Justfile <recipe>
# Paths use justfile_directory() so they work regardless of cwd.

path_local_base := justfile_directory()
path_local_zip := "{{path_local_base}}/zips"
path_local_csv := "{{path_local_base}}/csv"
path_local_parquet := "{{path_local_base}}/parquet"
path_s3_parquet := "s3://data.sb/census/pums/"

# Top-level defaults (override on CLI: just survey=acs1 end_year=2024 state=DC fetch)
survey := "acs5"
end_year := "2023"
state := "all"
record_type := "both"

# Fetch: download PUMS zips to zips/{{survey}}/{{end_year}}/
fetch:
    uv run python {{path_local_base}}/fetch_pums_csvs.py \
        --survey {{survey}} --end-year {{end_year}} --state {{state}} \
        --record-type {{record_type}} --output-dir {{path_local_zip}}

# Unzip: extract zips to csv/ (uses same survey, end_year as fetch)
unzip:
    #!/usr/bin/env bash
    set -euo pipefail
    ZIP_DIR="{{path_local_base}}/zips/{{survey}}/{{end_year}}"
    CSV_BASE="{{path_local_base}}/csv/{{survey}}/{{end_year}}"
    if [ ! -d "$ZIP_DIR" ]; then
        echo "Error: $ZIP_DIR not found. Run 'just fetch' first."
        exit 1
    fi
    for zipfile in "$ZIP_DIR"/*.zip; do
        if [ ! -f "$zipfile" ]; then continue; fi
        name=$(basename "$zipfile" .zip)
        rec_type="person"
        if [[ "$name" == csv_h* ]]; then rec_type="housing"; fi
        state_code=$(echo "$name" | sed 's/^csv_[ph]//' | tr 'a-z' 'A-Z')
        out_dir="$CSV_BASE/$rec_type/state=$state_code"
        mkdir -p "$out_dir"
        echo "Extracting: $name -> $out_dir"
        unzip -o -j "$zipfile" -d "$out_dir"
        rm -f "$out_dir"/*.pdf
    done
    echo "✓ Extracted to $CSV_BASE"

# Convert csv/ tree to parquet/
convert:
    uv run python {{path_local_base}}/convert_pums_csv_to_parquet.py \
        --input-dir {{path_local_csv}} --output-dir {{path_local_parquet}}

# Full pipeline: fetch -> unzip -> convert (uses top-level survey, end_year, state, record_type)
prepare: fetch unzip convert

# Sync parquet/ to S3 (not exercised in PR)
upload:
    aws s3 sync {{path_local_parquet}}/ {{path_s3_parquet}} --exclude "*" --include "*.parquet"

# Remove zips/, csv/, parquet/ under data/census/pums/
clean:
    rm -rf {{path_local_zip}} {{path_local_csv}} {{path_local_parquet}}
    echo "✓ Removed zips/, csv/, parquet/"
