# EIA-861 electric utility stats (yearly sales and customer counts via PUDL)
#
# Build IOU stats for all states to local state-partitioned parquet, then
# optionally upload to S3.
#
# Usage (from project root):
#   just -f data/eia/861/Justfile fetch       # Build all states to electric_utility_stats/
#   just -f data/eia/861/Justfile update     # fetch then upload to s3://data.sb/eia/861/electric_utility_stats/
#   just -f data/eia/861/Justfile fetch-state-stats NY   # One state as CSV to stdout (ad-hoc)
#   just -f data/eia/861/Justfile clean                  # Remove local parquet/

project_root := `git rev-parse --show-toplevel`
local_parquet_dir := project_root + "/data/eia/861/parquet/"
s3_parquet_dir := "s3://data.sb/eia/861/electric_utility_stats/"

# Build IOU stats for all states; write to electric_utility_stats/state=<state>/data.parquet
fetch:
    uv run python "{{project_root}}/data/eia/861/fetch_electric_utility_stat_parquets.py" --output-dir "{{local_parquet_dir}}"
    @echo "Wrote {{local_parquet_dir}}state=<state>/data.parquet"

# Upload local parquet to S3 (Hive-style partition state=<state>/data.parquet).
# sync only uploads new or changed files (compares size and mtime); objects already in S3
# with the same content are skipped. No --delete, so extra objects in S3 are left as-is.
upload:
    aws s3 sync "{{local_parquet_dir}}" "{{s3_parquet_dir}}"

# Print one state's stats as CSV to stdout (no --output-dir)
fetch-state-stats state:
    uv run python "{{project_root}}/data/eia/861/fetch_electric_utility_stat_parquets.py" {{state}}

# Remove local parquet/
clean:
    rm -rf "{{local_parquet_dir}}"
    @echo "Removed {{local_parquet_dir}}"
